{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of Terrorism on World Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets description\n",
    "\n",
    "[Global Terrorism Database](https://www.kaggle.com/START-UMD/gtd)\n",
    "\n",
    "This dataset contains information on more than 170,000 terrorit attacks. The Global Terrorism Database (GTD) is an open-source database including information on terrorist events around the world from 1970 through 2016 (with annual updates planned for the future). Unlike many other event databases, the GTD includes systematic data on domestic as well as international terrorist incidents that have occurred during this time period and now includes more than 170,000 cases. [Learn more.](http://start.umd.edu/gtd/)\n",
    "\n",
    "\n",
    "For more precise information about important details like definitions, collection methodoloy and others plese see the [GTD Codebook](http://start.umd.edu/gtd/downloads/Codebook.pdf).\n",
    "\n",
    "\n",
    "**Format** : CSV &nbsp;&nbsp;&nbsp;&nbsp; **Size** : 29 MB\n",
    "\n",
    "\n",
    "[World Development Indicators](https://www.kaggle.com/worldbank/world-development-indicators)\n",
    "\n",
    "The World Development Indicators from the World Bank contain over a thousand annual indicators of economic development from hundreds of countries around the world.\n",
    "\n",
    "Here's a [list of the available indicators](https://www.kaggle.com/benhamner/indicators-in-data) along with a [list of the available countries](https://www.kaggle.com/benhamner/countries-in-the-wdi-data).\n",
    "\n",
    "\n",
    "**Format** : SQLITE &nbsp;&nbsp;&nbsp;&nbsp; **Size** : 261 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main data cleaning and merging constraints\n",
    "Our project main purpose involves to get countries statisitcs from different indicators in relation to period where terrorist attacks occur. In order to do it, we have to our diposal two datasets: one with the different countries indicators which is in a really convenient SQLITE format, and the other which contains terrorist attacks and data related to it (like where it happened, which type of attack it is, ect.). <br />\n",
    "\n",
    "The first important part to deal with is to clean the datasets and merge them. Indeed, we would like to perform join operations between indicators and terrorist attacks. The main problem to overcome is the lack of \"agreed\" convention to uniquely identify countries between the two datasets. In the first one, all indicators use alpha3 codes (3 letters codes) to denote the countries. We also have a `Country` table which have the alpha2 code (2 letters) for each country. In the other dataset, the countries are identified using different codes but there wasn't a simple way to use it for merging. Moreover, the name of the countries used are not exactly the same for the two datasets.\n",
    "\n",
    "### Our solution \n",
    "The solution we found was to use the Google Maps API and to send requests to it with the name of the countries coming from the second dataset. This API is robust to country names spelling and can \"normalize\" them for us by returning the alpha2 code of the country given its name. <br />\n",
    "Here is the detail of our process:\n",
    "\n",
    "1\\. First, we got the `Country` table from the first dataset into CSV format and the attack dataset (which was already in CSV format) in order to process them easily with pandas. We only need the `Country` table from the frst dataset to perform the cleaning and merging step.\n",
    "\n",
    "\n",
    "2\\. Before normalizing the country names using the Google Map API, we needed to tackle some name ambiguity by hand for the *Republic of the Congo*. Indeed, in the attack dataset, this country is denoted as *People's Republic of the Congo* and the API couln't understand this name. So we replaced occurences of *People's Republic of the Congo* with *Republic of the Congo*. The issue was due to the similarity between the two countries *Congo* and *Republic of the Congo* and so the API couldn't figure out which one was *People's Republic of the Congo*.\n",
    "\n",
    "\n",
    "3\\. After resolving this ambiguity, we found each unique country name in the attacks dataset and we built a mapping going from country names to alpha2 code using the Google Map API. Then, we added a field called `iso_code` to the dataset with the alpha2 code using the mapping.\n",
    "\n",
    "\n",
    "4\\. After that, we had some last issue with some alpha2 code not found by the API, it was for Ireland and Namibia so we added them by hand.\n",
    "\n",
    "\n",
    "5\\. We deleted all attacks for which we had no country indicators: if a country of an attack was not present in the `Country` table, we deleted the row corresponding to this attack.\n",
    "\n",
    "\n",
    "6\\. At this point we were able to join the two datasets using the `Country` table, but since all indicators denote the country by an alpha3 code, we built a mapping going from alpha2 code to alpha3 code and we transformed the `iso_code` field so that it contains alpha3 code instead. If we did not performed this operation, each join we would make in the future would need an extra join to get the alpha3 code of an attack through the `Country` table. Now, we can only have a single join using directly the alpha3 code of the indicators and joining on the `iso_code` of the attacks table.\n",
    "\n",
    "\n",
    "7\\. To conclude, we saved the cleaned attacks dataset and added it to the SQLITE database so we can easily use SQL to query our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function take the name of a place on Earth (countries in our case) and make a request            \n",
    "# to Google Mas API in order to \"normalize\" the location and get the country code associated.           \n",
    "# This country code will help to join the terrorist attacks with the country indicators.                \n",
    "def getplace(place):                                                                                    \n",
    "    # Google Maps API base URL                                                                          \n",
    "    url = \"https://maps.googleapis.com/maps/api/geocode/json?\"                                          \n",
    "    # Add the parameters to the base URL                                                                \n",
    "    url += \"address=%s&sensor=false&key=AIzaSyAk90LdWroCWnkWwOVEB_89kAzz1uPCwo0\" % (quote_plus(place))  \n",
    "    v = urlopen(url).read()\n",
    "    j = json.loads(v)                                                                                   \n",
    "    try:                                                                                                \n",
    "        components = j['results'][0]['address_components']                                              \n",
    "        long = short = None                                                                             \n",
    "        for c in components:                                                                            \n",
    "            if \"country\" in c['types']:                                                                 \n",
    "                long = c['long_name']                                                                   \n",
    "                short = c['short_name']                                                                 \n",
    "        return long, short                                                                              \n",
    "    except:                                                                                             \n",
    "        # print('-------------', place)                                                                   \n",
    "        return None, None \n",
    "    \n",
    "    \n",
    "# Function that creates that normalize location in order to build a mapping which goes from             \n",
    "# location to country codes.                                                                             \n",
    "def mapping(n):                                                                                         \n",
    "    return n, getplace(n)[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d469451e11414fa7cc27c5dc38c174"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data folder path\n",
    "data_path = '../data/'\n",
    "# Get the number of cores available for parallelization\n",
    "n_cores = cpu_count()\n",
    "\n",
    "attacks = pd.read_csv(data_path + './globalterrorismdb_0617dist.csv', encoding=\"ISO-8859-1\")\n",
    "countries = pd.read_csv(data_path + './Country.csv')\n",
    "# Treat Congo names ambiguity\n",
    "attacks.loc[attacks.country_txt.str.contains('People\\'s Republic of the Congo'),\n",
    "        'country_txt'] = 'Republic of the Congo'\n",
    "# Get all unique names\n",
    "all_names = attacks.country_txt.unique()\n",
    "# Create a mapping from country names to alpha2 country codes\n",
    "pool = Pool(n_cores)\n",
    "name_to_code = tqdm_notebook(pool.imap_unordered(mapping, all_names),\n",
    "        total=all_names.size, desc='API calls')\n",
    "name_to_code = {k: v for k,v in name_to_code}\n",
    "# Add field with isocode\n",
    "attacks['iso_code'] = attacks.country_txt.apply(lambda x: name_to_code[x])\n",
    "# Treat special country with no match for iso_code\n",
    "attacks.loc[attacks.country_txt == 'Ireland','iso_code'] = 'IE'\n",
    "attacks.loc[attacks.country_txt == 'Namibia','iso_code'] = 'NA'\n",
    "# Delete attacks where no country indicators are available\n",
    "countries.loc[countries.CountryCode=='NAM', 'Alpha2Code'] = 'NA'\n",
    "countries.loc[countries.CountryCode=='KSV', 'Alpha2Code'] = 'XK'\n",
    "attacks = attacks[attacks.iso_code.isin(countries.Alpha2Code)]\n",
    "# Build a mapping from alpha2 country codes to alpha3 country codes \n",
    "alpha2_to_alpha3 = countries[['Alpha2Code', 'CountryCode']]\n",
    "alpha2_to_alpha3 = dict(alpha2_to_alpha3.apply(lambda x: (x.Alpha2Code, x.CountryCode),\n",
    "    axis=1).values)\n",
    "# Transform the iso_code field with the precedingly created mapping to get alpha3 code\n",
    "attacks.iso_code = attacks.iso_code.apply(lambda iso: alpha2_to_alpha3[iso])\n",
    "# Save dataframe\n",
    "attacks.to_csv(data_path + 'attacks_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
